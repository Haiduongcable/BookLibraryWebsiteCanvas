#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
submit_yoloworld.py

- Duy·ªát c√°c th∆∞ m·ª•c video (khung ·∫£nh .jpg) trong --frames_root
- V·ªõi m·ªói VIDEO_ID, ƒë·∫∑t l·ªõp YOLO-World theo quy t·∫Øc:
    BlackBox_0/BlackBox_1 -> "black box"
    CardboardBox_0        -> "cardboard box"
    (m·∫∑c ƒë·ªãnh) CamelCase  -> "camel case" d·∫°ng th∆∞·ªùng (LifeJacket -> "life jacket")
- M·ªói frame: ch·∫°y infer, l·∫•y bbox c√≥ confidence cao nh·∫•t SAU L·ªåC M√âP ·∫¢NH
- N·∫øu --use_tracking: l√†m m∆∞·ª£t EMA + d·ª± ƒëo√°n ti·∫øp khi miss t·ªëi ƒëa --track_max_age khung
  (k·∫øt qu·∫£ tracker c≈©ng b·ªã l·ªçc m√©p; n·∫øu vi ph·∫°m, coi nh∆∞ miss)
- Xu·∫•t submission.json theo schema y√™u c·∫ßu
- T√πy ch·ªçn l∆∞u visualize

Usage v√≠ d·ª•:
  No-tracking:
    python submit_yoloworld.py \
      --weights runs/finetune/yoloworld_custom/weights/best.pt \
      --frames_root data/public_test_frames \
      --out_dir out/submit_yw_no_track \
      --conf 0.001 --iou 0.7 --imgsz 640 --filter-box 0.02 --save_vis

  Tracking:
    python submit_yoloworld.py \
      --weights runs/finetune/yoloworld_custom/weights/best.pt \
      --frames_root data/public_test_frames \
      --out_dir out/submit_yw_track \
      --conf 0.001 --iou 0.7 --imgsz 640 \
      --use_tracking --track_alpha 0.6 --track_max_age 5 --track_conf_decay 0.9 \
      --filter-box 0.015 \
      --save_vis
"""

import argparse
import json
import logging
import re
from pathlib import Path

import cv2
import numpy as np
import torch
from tqdm import tqdm
from ultralytics import YOLOWorld


# -----------------------------
# Logging
# -----------------------------
def setup_logger():
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s | %(levelname)s | %(message)s"
    )
    return logging.getLogger("submit_yoloworld")


# -----------------------------
# IO helpers
# -----------------------------
def list_video_dirs(frames_root: Path):
    return [d for d in sorted(frames_root.iterdir()) if d.is_dir()]


def list_frames(video_dir: Path):
    return sorted(video_dir.glob("*.jpg"))


def parse_frame_idx(img_path: Path) -> int:
    stem = img_path.stem
    if "_frame_" in stem:
        try:
            return int(stem.split("_frame_")[-1])
        except Exception:
            pass
    m = re.search(r"(\d+)$", stem)
    return int(m.group(1)) if m else -1


def draw_bbox(im, bbox, label: str = None):
    x1, y1, x2, y2 = map(int, bbox)
    cv2.rectangle(im, (x1, y1), (x2, y2), (0, 255, 0), 2)
    if label:
        cv2.putText(
            im, label, (x1, max(0, y1 - 5)),
            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2
        )
    return im


# -----------------------------
# Class prompt resolver
# -----------------------------
SPECIAL_MAP = {
    "BlackBox": ["black box"],
    "CardboardBox": ["cardboard box"],
    "LifeJacket": ["life saver"]
    # C√≥ th·ªÉ m·ªü r·ªông th√™m n·∫øu c·∫ßn
}


def camel_to_words(s: str) -> str:
    spaced = re.sub(r"(?<!^)([A-Z])", r" \1", s).strip()
    return spaced.lower()


def video_id_to_prompts(video_id: str):
    base = video_id.split("_")[0]
    if base in SPECIAL_MAP:
        return SPECIAL_MAP[base]
    return [camel_to_words(base)]


# -----------------------------
# Edge filter utils
# -----------------------------
def clip_xyxy_to_image(xyxy: np.ndarray, w: int, h: int) -> np.ndarray:
    """Clip bbox [x1,y1,x2,y2] v√†o bi√™n ·∫£nh."""
    xyxy = xyxy.astype(float)
    xyxy[0] = np.clip(xyxy[0], 0, w - 1)
    xyxy[1] = np.clip(xyxy[1], 0, h - 1)
    xyxy[2] = np.clip(xyxy[2], 0, w - 1)
    xyxy[3] = np.clip(xyxy[3], 0, h - 1)
    return xyxy


def _edge_filter_mask(xyxy: np.ndarray, img_w: int, img_h: int, edge_ratio: float) -> np.ndarray:
    """
    T·∫°o mask True cho c√°c box H·ª¢P L·ªÜ (kh√¥ng s√°t m√©p).
    edge_ratio: 0..0.49; v√≠ d·ª• 0.02 -> y√™u c·∫ßu c√°ch m√©p ‚â• 2% k√≠ch th∆∞·ªõc ·∫£nh.
    """
    if edge_ratio <= 0.0 or xyxy.size == 0:
        return np.ones((xyxy.shape[0],), dtype=bool)

    mw = edge_ratio * img_w
    mh = edge_ratio * img_h
    x1 = xyxy[:, 0]
    y1 = xyxy[:, 1]
    x2 = xyxy[:, 2]
    y2 = xyxy[:, 3]
    # H·ª£p l·ªá n·∫øu c√°ch m√©p >= margin
    valid = (x1 >= mw) & (y1 >= mh) & (x2 <= (img_w - mw)) & (y2 <= (img_h - mh))
    return valid


def pick_best_bbox_with_edge(res, img_w: int, img_h: int, edge_ratio: float):
    """
    L·∫•y bbox c√≥ confidence cao nh·∫•t sau khi:
      - Clip v√†o bi√™n ·∫£nh
      - L·ªçc m√©p theo edge_ratio
    Tr·∫£ v·ªÅ dict ho·∫∑c None:
      { 'conf': float, 'xyxy': np.ndarray(4,), 'cls': int }
    """
    boxes = getattr(res, "boxes", None)
    if boxes is None or len(boxes) == 0:
        return None

    confs = boxes.conf.detach().cpu().numpy()
    xyxy = boxes.xyxy.detach().cpu().numpy()
    cls_ids = boxes.cls.detach().cpu().numpy().astype(int)

    # Clip to√†n b·ªô box v√†o ·∫£nh ƒë·ªÉ tr√°nh s·ªë √¢m / v∆∞·ª£t bi√™n
    for i in range(xyxy.shape[0]):
        xyxy[i] = clip_xyxy_to_image(xyxy[i], img_w, img_h)

    valid_mask = _edge_filter_mask(xyxy, img_w, img_h, edge_ratio)
    if not np.any(valid_mask):
        return None

    confs_v = confs[valid_mask]
    xyxy_v = xyxy[valid_mask]
    cls_v = cls_ids[valid_mask]
    best_i = int(np.argmax(confs_v))
    return {"conf": float(confs_v[best_i]), "xyxy": xyxy_v[best_i], "cls": int(cls_v[best_i])}


def passes_edge_filter(xyxy: np.ndarray, img_w: int, img_h: int, edge_ratio: float) -> bool:
    """Ki·ªÉm tra 1 bbox c√≥ pass edge filter kh√¥ng (sau khi ƒë√£ clip)."""
    if edge_ratio <= 0.0:
        return True
    xyxy = clip_xyxy_to_image(xyxy, img_w, img_h)
    mw = edge_ratio * img_w
    mh = edge_ratio * img_h
    x1, y1, x2, y2 = xyxy.tolist()
    return (x1 >= mw) and (y1 >= mh) and (x2 <= (img_w - mw)) and (y2 <= (img_h - mh))


# -----------------------------
# Tiny single-object tracker (EMA + linear prediction)
# -----------------------------
class TinySingleTracker:
    def __init__(self, alpha=0.6, max_age=5, conf_decay=0.90):
        self.alpha = float(alpha)
        self.max_age = int(max_age)
        self.conf_decay = float(conf_decay)
        self.reset()

    def reset(self):
        self.has_state = False
        self.bbox = None
        self.prev_bbox = None
        self.velocity = None
        self.conf = None
        self.missed = 0

    def update(self, det_bbox: np.ndarray, det_conf: float):
        det_bbox = det_bbox.astype(float)
        if not self.has_state:
            self.bbox = det_bbox
            self.prev_bbox = det_bbox
            self.velocity = np.zeros(4, dtype=float)
            self.conf = det_conf
            self.missed = 0
            self.has_state = True
            return self.bbox, self.conf

        new_bbox = self.alpha * det_bbox + (1.0 - self.alpha) * self.bbox
        self.velocity = new_bbox - self.bbox
        self.prev_bbox = self.bbox
        self.bbox = new_bbox
        self.conf = 0.5 * det_conf + 0.5 * (self.conf if self.conf is not None else det_conf)
        self.missed = 0
        return self.bbox, self.conf

    def predict(self):
        if not self.has_state:
            return None, None
        if self.missed >= self.max_age:
            self.reset()
            return None, None
        self.prev_bbox = self.bbox
        self.bbox = self.bbox + (self.velocity if self.velocity is not None else 0.0)
        if self.conf is None:
            self.conf = 0.0
        else:
            self.conf *= self.conf_decay
        self.missed += 1
        return self.bbox, self.conf


# -----------------------------
# Device handling & safe set_classes
# -----------------------------
def _normalize_device(dev_arg):
    if dev_arg is None:
        return None
    if isinstance(dev_arg, str):
        s = dev_arg.strip().lower()
        if s in ["cpu", "mps", "cuda", "cuda:0", "cuda:1", "cuda:2", "cuda:3"]:
            return s
        if s.isdigit():
            return f"cuda:{s}" if torch.cuda.is_available() else "cpu"
        return s
    if isinstance(dev_arg, int):
        return f"cuda:{dev_arg}" if torch.cuda.is_available() else "cpu"
    return None


def _ensure_txt_feats_on(model, device_str):
    """
    N·∫øu YOLO-World ƒë√£ t·∫°o text features (txt_feats), ƒë·∫£m b·∫£o ch√∫ng n·∫±m c√πng device v·ªõi model.
    """
    try:
        if hasattr(model, "model"):
            target = torch.device(device_str) if device_str is not None else next(model.model.parameters()).device
            if hasattr(model.model, "txt_feats") and model.model.txt_feats is not None:
                if isinstance(model.model.txt_feats, (list, tuple)):
                    model.model.txt_feats = [t.to(target) for t in model.model.txt_feats]
                else:
                    model.model.txt_feats = model.model.txt_feats.to(target)
    except Exception:
        # Kh√¥ng fail job n·∫øu kh√°c version Ultralytics
        pass


def set_yw_classes_safe(model, prompts, device_str):
    """
    Thi·∫øt l·∫≠p classes cho YOLO-World sao cho text feats + model c√πng device.
    """
    # B1: ƒë·∫£m b·∫£o model ƒëang ·ªü ƒë√∫ng device tr∆∞·ªõc khi set
    if device_str is not None:
        model.to(device_str)

    # B2: g·ªçi set_classes (∆∞u ti√™n cache_clip_model n·∫øu c√≥)
    try:
        model.set_classes(prompts, cache_clip_model=True)
    except TypeError:
        model.set_classes(prompts)

    # B3: √©p txt_feats v·ªÅ c√πng device v·ªõi model
    _ensure_txt_feats_on(model, device_str)


# -----------------------------
# Main
# -----------------------------
def main():
    ap = argparse.ArgumentParser(description="YOLO-World -> submission.json (+ optional visualize), tracking or not")
    ap.add_argument("--weights", required=True, help="Path YOLO-World weights (e.g., best.pt)")
    ap.add_argument("--frames_root", default="data/public_test_frames", help="Root ch·ª©a c√°c th∆∞ m·ª•c video")
    ap.add_argument("--out_dir", default="out/submission_pred_yw", help="Th∆∞ m·ª•c output (JSON + visualize)")
    ap.add_argument("--conf", type=float, default=0.001, help="Confidence threshold (n√™n th·∫•p cho recall)")
    ap.add_argument("--iou", type=float, default=0.7, help="IoU threshold NMS")
    ap.add_argument("--imgsz", type=int, default=640, help="Resize ·∫£nh khi predict")
    ap.add_argument("--filter-box", dest="filter_box", type=float, default=0.0,
                    help="T·ª∑ l·ªá bi√™n ƒë·ªÉ lo·∫°i box s√°t m√©p ·∫£nh (0.0..0.49). V√≠ d·ª• 0.02 = 2%% k√≠ch th∆∞·ªõc ·∫£nh.")
    ap.add_argument("--device", default=None, help="cpu | cuda | cuda:N | mps")
    ap.add_argument("--save_vis", action="store_true", help="L∆∞u visualize ·∫£nh overlay bbox")
    # Tracking options
    ap.add_argument("--use_tracking", action="store_true", help="B·∫≠t tracking m∆∞·ª£t (EMA + d·ª± ƒëo√°n ng·∫Øn h·∫°n)")
    ap.add_argument("--track_alpha", type=float, default=0.6, help="EMA alpha (0..1), cao -> b√°m detection nhi·ªÅu h∆°n")
    ap.add_argument("--track_max_age", type=int, default=5, help="Miss t·ªëi ƒëa N khung v·∫´n d·ª± ƒëo√°n bbox")
    ap.add_argument("--track_conf_decay", type=float, default=0.90, help="Gi·∫£m conf m·ªói khung khi d·ª± ƒëo√°n")
    args = ap.parse_args()

    if not (0.0 <= args.filter_box < 0.5):
        raise ValueError("--filter-box ph·∫£i trong kho·∫£ng [0.0, 0.49]")

    logger = setup_logger()

    # Ch·ªçn device th·ªëng nh·∫•t
    dev = _normalize_device(args.device)
    if dev and dev.startswith("cuda:") and torch.cuda.is_available():
        # ƒê·∫∑t current device (gi·∫£m kh·∫£ nƒÉng "index_select" mismatch)
        try:
            torch.cuda.set_device(int(dev.split(":")[1]))
        except Exception:
            pass

    # Load model v√† ƒë∆∞a v·ªÅ dev (m·ªôt l·∫ßn duy nh·∫•t)
    model = YOLOWorld(args.weights)
    logger.info(f"Loaded YOLO-World model: {args.weights}")
    if dev is not None:
        model.to(dev)

    frames_root = Path(args.frames_root)
    out_dir = Path(args.out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    out_json = out_dir / "submission.json"
    vis_root = out_dir / "visualize"
    if args.save_vis:
        vis_root.mkdir(parents=True, exist_ok=True)

    videos = list_video_dirs(frames_root)
    logger.info(f"Found {len(videos)} videos in {frames_root}")

    submission = []

    for vdir in tqdm(videos, desc="Videos"):
        video_id = vdir.name
        prompts = video_id_to_prompts(video_id)

        # set_classes an to√†n: ƒë·∫£m b·∫£o txt_feats c√πng device v·ªõi model
        set_yw_classes_safe(model, prompts, dev)
        logger.info(f"[{video_id}] classes={prompts}")

        frames = list_frames(vdir)
        if len(frames) == 0:
            submission.append({"video_id": video_id, "detections": []})
            continue

        if args.save_vis:
            out_vis_dir = vis_root / video_id
            out_vis_dir.mkdir(parents=True, exist_ok=True)

        detections = []
        tracker = TinySingleTracker(
            alpha=args.track_alpha,
            max_age=args.track_max_age,
            conf_decay=args.track_conf_decay
        ) if args.use_tracking else None

        for img_path in tqdm(frames, desc=f"{video_id}", leave=False):
            try:
                img = cv2.imread(str(img_path))
                if img is None:
                    raise FileNotFoundError(f"Cannot read image: {img_path}")
                H, W = img.shape[:2]

                # Quan tr·ªçng: KH√îNG truy·ªÅn device=... v√†o predict; d√πng device c·ªßa model
                results = model.predict(
                    source=img,
                    imgsz=args.imgsz,
                    conf=args.conf,
                    iou=args.iou,
                    stream=False,
                    verbose=False,
                    save=False
                )
                res = results[0]

                # pick detection sau l·ªçc m√©p
                best = pick_best_bbox_with_edge(res, W, H, edge_ratio=args.filter_box)

                used_bbox = None
                used_conf = None

                if tracker is None:
                    # No tracking: d√πng tr·ª±c ti·∫øp detection n·∫øu c√≥
                    if best is not None:
                        used_bbox = clip_xyxy_to_image(best["xyxy"], W, H)
                        if passes_edge_filter(used_bbox, W, H, args.filter_box):
                            used_conf = best["conf"]
                        else:
                            used_bbox, used_conf = None, None
                else:
                    # Tracking: update n·∫øu c√≥ detection h·ª£p l·ªá; n·∫øu kh√¥ng -> predict
                    if best is not None:
                        cand = clip_xyxy_to_image(best["xyxy"], W, H)
                        if passes_edge_filter(cand, W, H, args.filter_box):
                            used_bbox, used_conf = tracker.update(cand, best["conf"])
                        else:
                            used_bbox, used_conf = tracker.predict()
                    else:
                        used_bbox, used_conf = tracker.predict()

                    # V·ªõi bbox t·ª´ tracker (update/predict), clip + edge check
                    if used_bbox is not None:
                        used_bbox = clip_xyxy_to_image(used_bbox, W, H)
                        if not passes_edge_filter(used_bbox, W, H, args.filter_box):
                            used_bbox, used_conf = None, None

                # Append to submission (ch·ªâ khi c√≥ bbox)
                if used_bbox is not None:
                    x1, y1, x2, y2 = used_bbox.tolist()
                    frame_idx = parse_frame_idx(img_path)
                    detections.append({
                        "frame": frame_idx,
                        "x1": int(round(x1)),
                        "y1": int(round(y1)),
                        "x2": int(round(x2)),
                        "y2": int(round(y2)),
                    })

                # Visualize
                if args.save_vis:
                    vis_im = img.copy()
                    if used_bbox is not None:
                        label = f"{used_conf:.2f}" if used_conf is not None else None
                        vis_im = draw_bbox(vis_im, used_bbox, label)
                    else:
                        cv2.putText(
                            vis_im, "No det (edge-filtered)", (10, 30),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2
                        )
                    cv2.imwrite(str((out_vis_dir / img_path.name)), vis_im)

            except Exception as e:
                logger.warning(f"{img_path}: {e}")

        # Add to submission
        if len(detections) == 0:
            submission.append({"video_id": video_id, "detections": []})
        else:
            submission.append({
                "video_id": video_id,
                "detections": [{"bboxes": detections}]
            })

    # Save JSON
    with open(out_json, "w", encoding="utf-8") as f:
        json.dump(submission, f, ensure_ascii=False, indent=2)

    logger.info(f"‚úÖ Saved submission: {out_json}")
    if args.save_vis:
        logger.info(f"üñºÔ∏è Visualizations saved in: {vis_root}")


if __name__ == "__main__":
    main()
